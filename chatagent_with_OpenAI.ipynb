{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUeEL6rFx9VM",
        "outputId": "8b3b4ca7-5dc8-4300-c5a0-367a0e94a9ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/jim/resum/chatagent-scratch\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "#@title Allow your drive for local storage and set prefs (2 minutes, run once per session)\n",
        "import os, sys\n",
        "os.environ[\"LANGCHAIN_HANDLER\"] = \"langchain\"\n",
        "\n",
        "#@markdown OpenAI model name (recommend text-davinci-003)\n",
        "OPENAI_MODEL_NAME = \"text-davinci-003\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Drive folder for local storage, call it anything you want, will be created at top level of your drive\n",
        "OUTPUT_DIR = \"chatagent-scratch\" #@param {type:\"string\"}\n",
        "\n",
        "save_to_gdrive = False\n",
        "if save_to_gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/\" + OUTPUT_DIR\n",
        "else:\n",
        "    OUTPUT_DIR = \"~/resum/\" + OUTPUT_DIR\n",
        "\n",
        "#@markdown Your OpenAI api key (can be obtained from https://platform.openai.com/account/api-keys, this notebook will be cheap, but not free)\n",
        "OPENAI_API_KEY = \"\" #@param {type:\"string\"}\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "\n",
        "#@markdown Your SerpAPI api key (can be obtained from https://platform.openai.com/account/api-keys, this notebook will be cheap, but not free)\n",
        "SERPAPI_API_KEY = \"\" #@param {type:\"string\"}\n",
        "os.environ['SERPAPI_API_KEY'] = SERPAPI_API_KEY\n",
        "\n",
        "#@markdown Your Wolfram-Alpha api key (can be obtained from https://platform.openai.com/account/api-keys, this notebook will be cheap, but not free)\n",
        "WOLFRAM_ALPHA_APPID = \"\" #@param {type:\"string\"}\n",
        "os.environ['WOLFRAM_ALPHA_APPID'] = WOLFRAM_ALPHA_APPID\n",
        "\n",
        "!mkdir -p $OUTPUT_DIR\n",
        "\n",
        "%cd $OUTPUT_DIR\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdI5VYK1xoi-",
        "outputId": "7f612ecb-460f-438b-dfb9-d041a04a6b6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/jim/resum/chatagent-scratch\n",
            "git: 'lfs' is not a git command. See 'git --help'.\n",
            "\n",
            "The most similar command is\n",
            "\tlog\n",
            "fatal: destination path '/Users/jim/resum/chatagent-scratch' already exists and is not an empty directory.\n",
            "Requirement already satisfied: wolframalpha in /Users/jim/.pyenv/versions/3.10.1/lib/python3.10/site-packages (5.0.0)\n",
            "Requirement already satisfied: xmltodict in /Users/jim/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from wolframalpha) (0.13.0)\n",
            "Requirement already satisfied: more-itertools in /Users/jim/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from wolframalpha) (9.1.0)\n",
            "Requirement already satisfied: jaraco.context in /Users/jim/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from wolframalpha) (4.3.0)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
            "You should consider upgrading via the '/Users/jim/.pyenv/versions/3.10.1/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Dependencies look good.\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# show google drive integration in sidebar\n",
        "\n",
        "%cd $OUTPUT_DIR\n",
        "\n",
        "import os.path\n",
        "\n",
        "if not os.path.isfile(OUTPUT_DIR+\"/app.py\"):\n",
        "  !git lfs install\n",
        "  !git clone https://huggingface.co/spaces/JavaFXpert/Chat-GPT-LangChain $OUTPUT_DIR\n",
        "else:\n",
        "  print(\"Working directory looks good.\")\n",
        "\n",
        "try:\n",
        "  from langchain.llms import OpenAI\n",
        "  %pip install wolframalpha\n",
        "  print(\"Dependencies look good.\")\n",
        "except:\n",
        "  %cd $OUTPUT_DIR\n",
        "  !git pull\n",
        "  %pip install -r requirements.txt\n",
        "  %pip install wolframalpha\n",
        "  print(\"Dependencies look good.\")\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2QSUZ5nQ3aVr"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser, load_tools, initialize_agent\n",
        "from langchain.prompts import BaseChatPromptTemplate\n",
        "from langchain import SerpAPIWrapper, LLMChain\n",
        "from langchain.utilities.wolfram_alpha import WolframAlphaAPIWrapper\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from typing import List, Union\n",
        "from langchain.schema import AgentAction, AgentFinish, HumanMessage\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CnhmXPcIAr06"
      },
      "outputs": [],
      "source": [
        "# Define which tools the agent can use to answer user queries\n",
        "\n",
        "search = SerpAPIWrapper()\n",
        "wolfram = WolframAlphaAPIWrapper()\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name = \"Search with SerpAPI\",\n",
        "        func=search.run,\n",
        "        description=\"useful for when you need to answer questions about current events\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name = \"Ask WolframAPI\",\n",
        "        func=wolfram.run,\n",
        "        description=\"useful for when you need to answer a question about math, numbers, logic or quantities\"\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mPsPvnK64CxG"
      },
      "outputs": [],
      "source": [
        "#@title Parse Agent Output\n",
        "#@markdown This is just some regex stuff to see if 'Final Answer' appears and to extract what comes after it, nothing mysterious (at least no more mysterious than regex, haha)\n",
        "\n",
        "class CustomOutputParser(AgentOutputParser):\n",
        "    \n",
        "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
        "        # Check if agent should finish\n",
        "        if \"Final Answer:\" in llm_output:\n",
        "            return AgentFinish(\n",
        "                # Return values is generally always a dictionary with a single `output` key\n",
        "                # It is not recommended to try anything else at the moment :)\n",
        "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
        "                log=llm_output,\n",
        "            )\n",
        "        # Parse out the action and action input\n",
        "        regex = r\"Action: (.*?)[\\n]*Action Input:[\\s]*(.*)\"\n",
        "        match = re.search(regex, llm_output, re.DOTALL)\n",
        "        if not match:\n",
        "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
        "        action = match.group(1).strip()\n",
        "        action_input = match.group(2)\n",
        "        # Return the action and action input\n",
        "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "olkxN0lX38In"
      },
      "outputs": [],
      "source": [
        "#@title Format Each Step in the Lang Chain\n",
        "#@markdown This is fun. Here we can see what args the agent is using to make decisions at each step. All we're doing here is formatting them to make them nice to print at each step\n",
        "\n",
        "# Set up a prompt template\n",
        "class CustomPromptTemplate(BaseChatPromptTemplate):\n",
        "    # The template to use\n",
        "    template: str\n",
        "    # The list of tools available\n",
        "    tools: List[Tool]\n",
        "    \n",
        "    def format_messages(self, **kwargs) -> str:\n",
        "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
        "        # Format them in a particular way\n",
        "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
        "        thoughts = \"\"\n",
        "        for action, observation in intermediate_steps:\n",
        "            thoughts += action.log\n",
        "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
        "        # Set the agent_scratchpad variable to that value\n",
        "        kwargs[\"agent_scratchpad\"] = thoughts\n",
        "        # Create a tools variable from the list of tools provided\n",
        "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
        "        # Create a list of tool names for the tools provided\n",
        "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
        "        formatted = self.template.format(**kwargs)\n",
        "        return [HumanMessage(content=formatted)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2F9u3hEy364s"
      },
      "outputs": [],
      "source": [
        "#@title Customize the base LangChain template and add pirate talk!\n",
        "\n",
        "# THIS IS WHERE THE MAGIC HAPPENS\n",
        "template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "{agent_scratchpad}\"\"\"\n",
        "\n",
        "prompt = CustomPromptTemplate(\n",
        "    template=template,\n",
        "    tools=tools,\n",
        "    input_variables=[\"input\", \"intermediate_steps\"]\n",
        ")\n",
        "\n",
        "# Use the substitutions below to make the agent talk like a pirate\n",
        "# ...best you can, but speaking as a pirate might speak\n",
        "# ...Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Arg\"s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WSPRUFNH4PXL"
      },
      "outputs": [],
      "source": [
        "output_parser = CustomOutputParser()\n",
        "\n",
        "# LLM chain consisting of the LLM and a prompt\n",
        "llm = ChatOpenAI(temperature=0) # can easily set local model here as well\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "tool_names = [tool.name for tool in tools]\n",
        "\n",
        "agent = LLMSingleActionAgent(\n",
        "    llm_chain=llm_chain, \n",
        "    output_parser=output_parser,\n",
        "    stop=[\"\\nObservation:\"],\n",
        "    allowed_tools=tool_names\n",
        ")\n",
        "\n",
        "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "asjdE7i_K1na",
        "outputId": "359925b9-88be-4a8a-b6c9-2b2b91feec98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: This question requires simple counting and doesn't require any API. \n",
            "Action: Count the words in the sentence and find the eighth word.\n",
            "Action Input: Count the words in the sentence.\u001b[0m\n",
            "\n",
            "Observation:Count the words in the sentence and find the eighth word. is not a valid tool, try another one.\u001b[32;1m\u001b[1;3mOops, I don't need any API for this question. Let me just count the eighth word manually.\n",
            "Action: Count the eighth word in the sentence.\n",
            "Action Input: Manually count the eighth word.\u001b[0m\n",
            "\n",
            "Observation:Count the eighth word in the sentence. is not a valid tool, try another one.\u001b[32;1m\u001b[1;3mI don't need any API for this question, let me just count the eighth word manually.\n",
            "Action: Count the eighth word in the sentence.\n",
            "Action Input: Manually count the eighth word.\u001b[0m\n",
            "\n",
            "Observation:Count the eighth word in the sentence. is not a valid tool, try another one.\u001b[32;1m\u001b[1;3mOops, my previous actions were not necessary. Let me just count the eighth word in the sentence.\n",
            "Action: Count the eighth word in the sentence.\n",
            "Action Input: Read and count the eighth word in the sentence.\u001b[0m\n",
            "\n",
            "Observation:Count the eighth word in the sentence. is not a valid tool, try another one.\u001b[32;1m\u001b[1;3mCounting the eighth word is easy, let me just do it again.\n",
            "Action: Count the eighth word in the sentence.\n",
            "Action Input: Read and count the eighth word in the sentence.\u001b[0m\n",
            "\n",
            "Observation:Count the eighth word in the sentence. is not a valid tool, try another one.\u001b[32;1m\u001b[1;3mThis is getting repetitive. Let me just count the eighth word in the sentence for the final time.\n",
            "Action: Count the eighth word in the sentence.\n",
            "Action Input: Read and count the eighth word in the sentence.\u001b[0m\n",
            "\n",
            "Observation:Count the eighth word in the sentence. is not a valid tool, try another one."
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Could not parse LLM output: `The final answer is \"in\".`",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# agent_executor.run(\"How many people live in canada as of 2023?\")\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# agent_executor.run(\"What is 37 x 455938\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m# don't forget to make it talk like a pirate...\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m# explain llm-math SHOW CUSTOM PROMPT (non-deterministic programming): # https://python.langchain.com/en/latest/modules/chains/examples/llm_math.html\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m agent_executor\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\u001b[39;49m\u001b[39mWhat is the eighth word in this sentence?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.1/lib/python3.10/site-packages/langchain/chains/base.py:213\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    212\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 213\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m])[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.1/lib/python3.10/site-packages/langchain/chains/base.py:116\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_end(outputs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m    118\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.1/lib/python3.10/site-packages/langchain/chains/base.py:113\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    108\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    109\u001b[0m     inputs,\n\u001b[1;32m    110\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs)\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.1/lib/python3.10/site-packages/langchain/agents/agent.py:637\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations):\n\u001b[0;32m--> 637\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[1;32m    638\u001b[0m         name_to_tool_map, color_mapping, inputs, intermediate_steps\n\u001b[1;32m    639\u001b[0m     )\n\u001b[1;32m    640\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m    641\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(next_step_output, intermediate_steps)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.1/lib/python3.10/site-packages/langchain/agents/agent.py:553\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Take a single step in the thought-action-observation loop.\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \n\u001b[1;32m    550\u001b[0m \u001b[39mOverride this to take control of how the agent makes and acts on choices.\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \u001b[39m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49mplan(intermediate_steps, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m    554\u001b[0m \u001b[39m# If the tool chosen is the finishing tool, then we end and return.\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, AgentFinish):\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.1/lib/python3.10/site-packages/langchain/agents/agent.py:176\u001b[0m, in \u001b[0;36mLLMSingleActionAgent.plan\u001b[0;34m(self, intermediate_steps, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Given input, decided what to do.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \n\u001b[1;32m    165\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39m    Action specifying what tool to use.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    173\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39mrun(\n\u001b[1;32m    174\u001b[0m     intermediate_steps\u001b[39m=\u001b[39mintermediate_steps, stop\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    175\u001b[0m )\n\u001b[0;32m--> 176\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_parser\u001b[39m.\u001b[39;49mparse(output)\n",
            "Cell \u001b[0;32mIn[5], line 19\u001b[0m, in \u001b[0;36mCustomOutputParser.parse\u001b[0;34m(self, llm_output)\u001b[0m\n\u001b[1;32m     17\u001b[0m match \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msearch(regex, llm_output, re\u001b[39m.\u001b[39mDOTALL)\n\u001b[1;32m     18\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m match:\n\u001b[0;32m---> 19\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not parse LLM output: `\u001b[39m\u001b[39m{\u001b[39;00mllm_output\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m action \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mgroup(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m     21\u001b[0m action_input \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mgroup(\u001b[39m2\u001b[39m)\n",
            "\u001b[0;31mValueError\u001b[0m: Could not parse LLM output: `The final answer is \"in\".`"
          ]
        }
      ],
      "source": [
        "# agent_executor.run(\"How many people live in canada as of 2023?\")\n",
        "# agent_executor.run(\"What is 37 x 455938\")\n",
        "\n",
        "# agent_executor.run(\"Write me a poem about cats\")\n",
        "\n",
        "# SPEAKER'S NOTES\n",
        "# don't forget to make it talk like a pirate...\n",
        "# explain llm-math SHOW CUSTOM PROMPT (non-deterministic programming): # https://python.langchain.com/en/latest/modules/chains/examples/llm_math.html\n",
        "\n",
        "agent_executor.run(\"What is the eighth word in this sentence?\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LiqlwEUr6JFY"
      },
      "source": [
        "===================================================================\n",
        "==================================================================="
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1Qy95GpXV2w"
      },
      "source": [
        "\n",
        "# TODOs\n",
        "\n",
        "Implement embeddings and vector search\n",
        "```\n",
        "def FaissWrapper():\n",
        "    foo = \"bar\"\n",
        "    topic_search = FaissWrapper()\n",
        "    tools = [\n",
        "      Tool(\n",
        "        name = \"Search Sources\",\n",
        "        func=topic_search.run,\n",
        "        description=\"useful for when you need to search embeddings\"\n",
        "    )\n",
        "]\n",
        "```\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
